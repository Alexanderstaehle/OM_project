{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODOs:\n",
    "\n",
    "- final train and val loss (best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Google colab initialization\n",
    "\n",
    "For Google colab uncomment these lines and run them to access your drive or try the second way (not tested)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#USE_COLAB = True\n",
    "#\n",
    "#if USE_COLAB:\n",
    "#    from google.colab import drive\n",
    "#\n",
    "#    drive.mount('/content/drive')\n",
    "#    import sys\n",
    "#\n",
    "#    sys.path.insert(1, r'/content/drive/My Drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Other try\n",
    "# !git clone https://github.com/Alexanderstaehle/OM_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append(\"OM_project\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from utils import ml_utils, visualization, data_loading, tf_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RETRAIN_FLAG = True\n",
    "#RETRAIN_FLAG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_bs = lambda bs: ml_utils.path_from_filename(f'model_fixed_lr_diff_bs_{bs}', format_=\"tf\")\n",
    "filename_bs_lr = lambda bs, lr: ml_utils.path_from_filename(f'model_lr_{lr}_diff_bs_{bs}', format_=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_bs_opt_sam = lambda bs, opt, sam: ml_utils.path_from_filename(\n",
    "    f'model_fixed_lr_diff_bs_{bs}_opt_{opt}_sam_{sam}', format_=\"tf\")\n",
    "filename_bs_lr_opt_sam = lambda bs, lr, opt, sam: ml_utils.path_from_filename(\n",
    "    f'model_lr_{lr}_diff_bs_{bs}_opt_{opt}_sam_{sam}', format_=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_load_sam_model_weights(train, optimizer, filename, adaptive=False, rho=0.05):\n",
    "    model = tf_models.build_simple_cnn_sam(train, optimizer, adaptive, rho)\n",
    "    model.load_weights(filename)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_load_model_weights(train, optimizer, filename):\n",
    "    model = tf_models.build_and_compile_simple_cnn(train, optimizer)\n",
    "    model.load_weights(filename)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback_for_filename(filename):\n",
    "    train_callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\", patience=10,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filename,\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            verbose=1,\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    return train_callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loading.initialize_env()\n",
    "sns.set_theme()\n",
    "color_map = sns.color_palette(as_cmap=True)\n",
    "ml_utils.check_tpu_gpu()\n",
    "# dataset_name = 'MNIST'\n",
    "dataset_name = 'Fashion_MNIST'\n",
    "EPOCHS = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "models = {}\n",
    "models_states = {}\n",
    "sharpnesses = ml_utils.init_sharpnesses_dict()\n",
    "initial_weights = ml_utils.init_initial_weights_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Different batch sizes with fixed learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with sharpness aware minimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD with Momentum + SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batch_sizes = [32, 64, 128, 256, 512, 1024]\n",
    "lr = 0.001\n",
    "training_epochs = EPOCHS\n",
    "\n",
    "key = ('fixed', 'sgd', 'sam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if not RETRAIN_FLAG:\n",
    "    for batch_size in batch_sizes:\n",
    "        # Read training data\n",
    "        train, validation = data_loading.load_batched_and_resized_dataset(dataset_name=dataset_name,\n",
    "                                                                          batch_size=batch_size,\n",
    "                                                                          img_size=32)\n",
    "        optimizer = keras.optimizers.SGD(learning_rate=lr, momentum=0.9)\n",
    "        model = build_and_load_sam_model_weights(train, optimizer, filename_bs_opt_sam(batch_size, \"SGD-MOM\", \"SAM\"))\n",
    "        models[key + (batch_size,)] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN_FLAG:\n",
    "    models_states[key] = {}\n",
    "    for batch_size in batch_sizes:\n",
    "        with tf.distribute.MirroredStrategy().scope():\n",
    "            # Read training data\n",
    "            train, validation = data_loading.load_batched_and_resized_dataset(dataset_name=dataset_name,\n",
    "                                                                              batch_size=batch_size,\n",
    "                                                                              img_size=32)\n",
    "\n",
    "            optimizer = keras.optimizers.SGD(learning_rate=lr, momentum=0.9)\n",
    "            model = tf_models.build_simple_cnn_sam(train, optimizer)\n",
    "            train_callbacks = callback_for_filename(filename_bs_opt_sam(batch_size, \"SGD-MOM\", \"SAM\"))\n",
    "\n",
    "            models_states[key][batch_size] = ml_utils.train_model(model, train, validation, epochs=training_epochs,\n",
    "                                                                  extra_callbacks=train_callbacks, verbose=1)\n",
    "            models[key + (batch_size,)] = model\n",
    "            initial_weights[key + (batch_size,)] = model.get_weights()\n",
    "            ml_utils.save_initial_weights_dict(initial_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fixed_lr_state_filename = 'model_fixed_lr_diff_bs_state'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN_FLAG:\n",
    "    ml_utils.save_model_state(models_states[key], fixed_lr_state_filename)\n",
    "else:\n",
    "    models_states[key] = ml_utils.load_model_state(fixed_lr_state_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "visualization.plot_loss_by_param(models_states[key], 'batch size with fixed learning rate', 'fixed_lr_diff_bs_SGD_SAM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Sharpness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if RETRAIN_FLAG:\n",
    "    for batch_size in batch_sizes:\n",
    "        model = models[key + (batch_size,)]\n",
    "\n",
    "        sharpness_bs = visualization.get_sharpness(model.base_model, train)\n",
    "        sharpnesses[key + (batch_size,)] = sharpness_bs\n",
    "\n",
    "        ml_utils.save_sharpnesses_dict(sharpnesses)\n",
    "\n",
    "if not RETRAIN_FLAG:\n",
    "    sharpnesses = ml_utils.load_sharpnesses_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_sharpness(batch_sizes, sharpnesses, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance from initial weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_distance_from_initial_weight(models, initial_weights, batch_sizes, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mean_times, convergence_epochs, overall_training_times = visualization.extract_times_for_batch_sizes(models_states,\n",
    "                                                                                                     batch_sizes, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_mean_time_per_epoch(batch_sizes, mean_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.histogram_num_of_train_epochs_until_conv(batch_sizes, convergence_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.histogram_overall_time_until_end_of_epochs(batch_sizes, overall_training_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_sharpness_times_runtime(batch_sizes, overall_training_times, sharpnesses, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD with Momentum + ASAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "models_dict_fixed_sgd_asam = {}\n",
    "batch_sizes = [32, 64, 128, 256, 512, 1024]\n",
    "#batch_sizes = [32]\n",
    "lr = 0.001\n",
    "training_epochs = EPOCHS\n",
    "\n",
    "key = ('fixed', 'sgd', 'asam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_by_batch_size_fixed_lr_sgd_asam = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not RETRAIN_FLAG:\n",
    "    for batch_size in batch_sizes:\n",
    "        # Read training data\n",
    "        train, validation = data_loading.load_batched_and_resized_dataset(dataset_name=dataset_name,\n",
    "                                                                          batch_size=batch_size,\n",
    "                                                                          img_size=32)\n",
    "        optimizer = keras.optimizers.SGD(learning_rate=lr, momentum=0.9)\n",
    "        model = build_and_load_sam_model_weights(train, optimizer, filename_bs_opt_sam(batch_size, \"SGD-MOM\", \"ASAM\"))\n",
    "        models_by_batch_size_fixed_lr_sgd_asam[batch_size] = model\n",
    "        models[key + (batch_size,)] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN_FLAG:\n",
    "    models_states[key] = {}\n",
    "    for batch_size in batch_sizes:\n",
    "        with tf.distribute.MirroredStrategy().scope():\n",
    "            # Read training data\n",
    "            train, validation = data_loading.load_batched_and_resized_dataset(dataset_name=dataset_name,\n",
    "                                                                              batch_size=batch_size,\n",
    "                                                                              img_size=32)\n",
    "\n",
    "            optimizer = keras.optimizers.SGD(learning_rate=lr, momentum=0.9)\n",
    "            model = tf_models.build_simple_cnn_sam(train, optimizer, adaptive=True, rho=2.0)\n",
    "            train_callbacks = callback_for_filename(filename_bs_opt_sam(batch_size, \"SGD-MOM\", \"ASAM\"))\n",
    "\n",
    "            models_states[key][batch_size] = ml_utils.train_model(model, train, validation, epochs=training_epochs,\n",
    "                                                                  extra_callbacks=train_callbacks, verbose=1)\n",
    "            models[key + (batch_size,)] = model\n",
    "            initial_weights[key + (batch_size,)] = model.get_weights()\n",
    "            ml_utils.save_initial_weights_dict(initial_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_lr_sgd_asam_state_filename = 'model_fixed_lr_diff_bs_sgd_asam_state'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN_FLAG:\n",
    "    ml_utils.save_model_state(models_states[key], fixed_lr_sgd_asam_state_filename)\n",
    "else:\n",
    "    models_states[key] = ml_utils.load_model_state(fixed_lr_sgd_asam_state_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_loss_by_param(models_states[key], 'batch size with fixed learning rate, SGD and ASAM',\n",
    "                                 'model_fixed_lr_diff_bs_sgd_asam_state')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Sharpness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if RETRAIN_FLAG:\n",
    "    for batch_size in batch_sizes:\n",
    "        model = models[key + (batch_size,)]\n",
    "\n",
    "        sharpness_bs = visualization.get_sharpness(model.base_model, train)\n",
    "        sharpnesses[key + (batch_size,)] = sharpness_bs\n",
    "\n",
    "        ml_utils.save_sharpnesses_dict(sharpnesses)\n",
    "\n",
    "if not RETRAIN_FLAG:\n",
    "    sharpnesses = ml_utils.load_sharpnesses_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_sharpness(batch_sizes, sharpnesses, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance from initial weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_distance_from_initial_weight(models, initial_weights, batch_sizes, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_times, convergence_epochs, overall_training_times = visualization.extract_times_for_batch_sizes(models_states,\n",
    "                                                                                                     batch_sizes, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_mean_time_per_epoch(batch_sizes, mean_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.histogram_num_of_train_epochs_until_conv(batch_sizes, convergence_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.histogram_overall_time_until_end_of_epochs(batch_sizes, overall_training_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_sharpness_times_runtime(batch_sizes, overall_training_times, sharpnesses, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADAM + SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [32, 64, 128, 256, 512, 1024]\n",
    "lr = 0.001\n",
    "training_epochs = EPOCHS\n",
    "\n",
    "key = ('fixed', 'adam', 'sam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not RETRAIN_FLAG:\n",
    "    for batch_size in batch_sizes:\n",
    "        # Read training data\n",
    "        train, validation = data_loading.load_batched_and_resized_dataset(dataset_name=dataset_name,\n",
    "                                                                          batch_size=batch_size,\n",
    "                                                                          img_size=32)\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "        model = build_and_load_sam_model_weights(train, optimizer, filename_bs_opt_sam(batch_size, \"ADAM\", \"SAM\"))\n",
    "        models[key + (batch_size,)] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN_FLAG:\n",
    "    models_states[key] = {}\n",
    "    for batch_size in batch_sizes:\n",
    "        with tf.distribute.MirroredStrategy().scope():\n",
    "            # Read training data\n",
    "            train, validation = data_loading.load_batched_and_resized_dataset(dataset_name=dataset_name,\n",
    "                                                                              batch_size=batch_size,\n",
    "                                                                              img_size=32)\n",
    "\n",
    "            optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "            model = tf_models.build_simple_cnn_sam(train, optimizer)\n",
    "            train_callbacks = callback_for_filename(filename_bs_opt_sam(batch_size, \"ADAM\", \"SAM\"))\n",
    "\n",
    "            models_states[key][batch_size] = ml_utils.train_model(model, train, validation, epochs=training_epochs,\n",
    "                                                                  extra_callbacks=train_callbacks, verbose=1)\n",
    "            models[key + (batch_size,)] = model\n",
    "            initial_weights[key + (batch_size,)] = model.get_weights()\n",
    "            ml_utils.save_initial_weights_dict(initial_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_filename = 'model_fixed_lr_diff_bs_adam_sam_state'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN_FLAG:\n",
    "    ml_utils.save_model_state(models_states[key], state_filename)\n",
    "else:\n",
    "    models_states[key] = ml_utils.load_model_state(state_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_loss_by_param(models_states[key], 'batch size with fixed learning rate, ADAM and SAM',\n",
    "                                 state_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sharpness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN_FLAG:\n",
    "    for batch_size in batch_sizes:\n",
    "        model = models[key + (batch_size,)]\n",
    "\n",
    "        sharpness_bs = visualization.get_sharpness(model.base_model, train)\n",
    "        sharpnesses[key + (batch_size,)] = sharpness_bs\n",
    "\n",
    "        ml_utils.save_sharpnesses_dict(sharpnesses)\n",
    "\n",
    "if not RETRAIN_FLAG:\n",
    "    sharpnesses = ml_utils.load_sharpnesses_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "visualization.plot_sharpness(batch_sizes, sharpnesses, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance from initial weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_distance_from_initial_weight(models, initial_weights, batch_sizes, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_times, convergence_epochs, overall_training_times = visualization.extract_times_for_batch_sizes(models_states,\n",
    "                                                                                                     batch_sizes, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_mean_time_per_epoch(batch_sizes, mean_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.histogram_num_of_train_epochs_until_conv(batch_sizes, convergence_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "visualization.histogram_overall_time_until_end_of_epochs(batch_sizes, overall_training_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_sharpness_times_runtime(batch_sizes, overall_training_times, sharpnesses, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### ADAM + ASAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [32, 64, 128, 256, 512, 1024]\n",
    "lr = 0.001\n",
    "training_epochs = EPOCHS\n",
    "\n",
    "key = ('fixed', 'adam', 'asam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not RETRAIN_FLAG:\n",
    "    for batch_size in batch_sizes:\n",
    "        # Read training data\n",
    "        train, validation = data_loading.load_batched_and_resized_dataset(dataset_name=dataset_name,\n",
    "                                                                          batch_size=batch_size,\n",
    "                                                                          img_size=32)\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "        model = build_and_load_sam_model_weights(train, optimizer, filename_bs_opt_sam(batch_size, \"ADAM\", \"ASAM\"))\n",
    "        models[key + (batch_size,)] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN_FLAG:\n",
    "    models_states[key] = {}\n",
    "    for batch_size in batch_sizes:\n",
    "        with tf.distribute.MirroredStrategy().scope():\n",
    "            # Read training data\n",
    "            train, validation = data_loading.load_batched_and_resized_dataset(dataset_name=dataset_name,\n",
    "                                                                              batch_size=batch_size,\n",
    "                                                                              img_size=32)\n",
    "\n",
    "            optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "            model = tf_models.build_simple_cnn_sam(train, optimizer, adaptive=True, rho=2.0)\n",
    "            train_callbacks = callback_for_filename(filename_bs_opt_sam(batch_size, \"ADAM\", \"ASAM\"))\n",
    "\n",
    "            models_states[key][batch_size] = ml_utils.train_model(model, train, validation, epochs=training_epochs,\n",
    "                                                                  extra_callbacks=train_callbacks, verbose=1)\n",
    "            models[key + (batch_size,)] = model\n",
    "            initial_weights[key + (batch_size,)] = model.get_weights()\n",
    "            ml_utils.save_initial_weights_dict(initial_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_filename = 'model_fixed_lr_diff_bs_adam_asam_state'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN_FLAG:\n",
    "    ml_utils.save_model_state(models_states[key], state_filename)\n",
    "else:\n",
    "    models_states[key] = ml_utils.load_model_state(state_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "visualization.plot_loss_by_param(models_states[key], 'batch size with fixed learning rate, ADAM and ASAM',\n",
    "                                 state_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Sharpness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if RETRAIN_FLAG:\n",
    "    for batch_size in batch_sizes:\n",
    "        model = models[key + (batch_size,)]\n",
    "\n",
    "        sharpness_bs = visualization.get_sharpness(model.base_model, train)\n",
    "        sharpnesses[key + (batch_size,)] = sharpness_bs\n",
    "\n",
    "        ml_utils.save_sharpnesses_dict(sharpnesses)\n",
    "\n",
    "if not RETRAIN_FLAG:\n",
    "    sharpnesses = ml_utils.load_sharpnesses_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_sharpness(batch_sizes, sharpnesses, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance from initial weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_distance_from_initial_weight(models, initial_weights, batch_sizes, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_times, convergence_epochs, overall_training_times = visualization.extract_times_for_batch_sizes(models_states,\n",
    "                                                                                                     batch_sizes, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_mean_time_per_epoch(batch_sizes, mean_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "visualization.histogram_num_of_train_epochs_until_conv(batch_sizes, convergence_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.histogram_overall_time_until_end_of_epochs(batch_sizes, overall_training_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_sharpness_times_runtime(batch_sizes, overall_training_times, sharpnesses, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## without sharpness aware minimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### SGD with Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [32, 64, 128, 256, 512, 1024]\n",
    "lr = 0.001\n",
    "training_epochs = EPOCHS\n",
    "\n",
    "key = ('fixed', 'sgd', 'none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not RETRAIN_FLAG:\n",
    "    for batch_size in batch_sizes:\n",
    "        # Read training data\n",
    "        train, validation = data_loading.load_batched_and_resized_dataset(dataset_name=dataset_name,\n",
    "                                                                          batch_size=batch_size,\n",
    "                                                                          img_size=32)\n",
    "        optimizer = keras.optimizers.SGD(learning_rate=lr, momentum=0.9)\n",
    "        model = build_and_load_model_weights(train, optimizer, filename_bs_opt_sam(batch_size, \"SGD-MOM\", \"NONE\"))\n",
    "        models[key + (batch_size,)] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN_FLAG:\n",
    "    models_states[key] = {}\n",
    "    for batch_size in batch_sizes:\n",
    "        with tf.distribute.MirroredStrategy().scope():\n",
    "            # Read training data\n",
    "            train, validation = data_loading.load_batched_and_resized_dataset(dataset_name=dataset_name,\n",
    "                                                                              batch_size=batch_size,\n",
    "                                                                              img_size=32)\n",
    "\n",
    "            optimizer = keras.optimizers.SGD(learning_rate=lr, momentum=0.9)\n",
    "            model = tf_models.build_and_compile_simple_cnn(train, optimizer)\n",
    "            train_callbacks = callback_for_filename(filename_bs_opt_sam(batch_size, \"SGD-MOM\", \"NONE\"))\n",
    "\n",
    "            models_states[key][batch_size] = ml_utils.train_model(model, train, validation, epochs=training_epochs,\n",
    "                                                                  extra_callbacks=train_callbacks, verbose=1)\n",
    "            models[key + (batch_size,)] = model\n",
    "            initial_weights[key + (batch_size,)] = model.get_weights()\n",
    "            ml_utils.save_initial_weights_dict(initial_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "state_filename = 'model_fixed_lr_diff_bs_noSAM_state'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN_FLAG:\n",
    "    ml_utils.save_model_state(models_states[key], state_filename)\n",
    "else:\n",
    "    models_states[key] = ml_utils.load_model_state(state_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_loss_by_param(models_states[key], 'batch size with fixed learning rate without SAM',\n",
    "                                 'fixed_lr_diff_bs_SGD_noSAM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sharpness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN_FLAG:\n",
    "    for batch_size in batch_sizes:\n",
    "        model = models[key + (batch_size,)]\n",
    "\n",
    "        sharpness_bs = visualization.get_sharpness(model, train)\n",
    "        sharpnesses[key + (batch_size,)] = sharpness_bs\n",
    "\n",
    "        ml_utils.save_sharpnesses_dict(sharpnesses)\n",
    "\n",
    "if not RETRAIN_FLAG:\n",
    "    sharpnesses = ml_utils.load_sharpnesses_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_sharpness(batch_sizes, sharpnesses, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance from initial weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_distance_from_initial_weight(models, initial_weights, batch_sizes, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_times, convergence_epochs, overall_training_times = visualization.extract_times_for_batch_sizes(models_states,\n",
    "                                                                                                     batch_sizes, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "visualization.plot_mean_time_per_epoch(batch_sizes, mean_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.histogram_num_of_train_epochs_until_conv(batch_sizes, convergence_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.histogram_overall_time_until_end_of_epochs(batch_sizes, overall_training_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_sharpness_times_runtime(batch_sizes, overall_training_times, sharpnesses, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [32, 64, 128, 256, 512, 1024]\n",
    "lr = 0.001\n",
    "training_epochs = EPOCHS\n",
    "\n",
    "key = ('fixed', 'adam', 'none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not RETRAIN_FLAG:\n",
    "    for batch_size in batch_sizes:\n",
    "        # Read training data\n",
    "        train, validation = data_loading.load_batched_and_resized_dataset(dataset_name=dataset_name,\n",
    "                                                                          batch_size=batch_size,\n",
    "                                                                          img_size=32)\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "        model = build_and_load_model_weights(train, optimizer, filename_bs_opt_sam(batch_size, \"ADAM\", \"NONE\"))\n",
    "        models[key + (batch_size,)] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN_FLAG:\n",
    "    models_states[key] = {}\n",
    "    for batch_size in batch_sizes:\n",
    "        with tf.distribute.MirroredStrategy().scope():\n",
    "            # Read training data\n",
    "            train, validation = data_loading.load_batched_and_resized_dataset(dataset_name=dataset_name,\n",
    "                                                                              batch_size=batch_size,\n",
    "                                                                              img_size=32)\n",
    "\n",
    "            optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "            model = tf_models.build_and_compile_simple_cnn(train, optimizer)\n",
    "            train_callbacks = callback_for_filename(filename_bs_opt_sam(batch_size, \"ADAM\", \"NONE\"))\n",
    "\n",
    "            models_states[key][batch_size] = ml_utils.train_model(model, train, validation, epochs=training_epochs,\n",
    "                                                                  extra_callbacks=train_callbacks, verbose=1)\n",
    "            models[key + (batch_size,)] = model\n",
    "            initial_weights[key + (batch_size,)] = model.get_weights()\n",
    "            ml_utils.save_initial_weights_dict(initial_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "state_filename = 'model_fixed_lr_diff_bs_ADAM_noSAM_state'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN_FLAG:\n",
    "    ml_utils.save_model_state(models_states[key], state_filename)\n",
    "else:\n",
    "    models_states[key] = ml_utils.load_model_state(state_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualization.plot_loss_by_param(models_states[key], 'batch size with fixed learning rate without SAM and ADAM',\n",
    "                                 'fixed_lr_diff_bs_ADAM_noSAM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sharpness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN_FLAG:\n",
    "    for batch_size in batch_sizes:\n",
    "        model = models[key + (batch_size,)]\n",
    "\n",
    "        sharpness_bs = visualization.get_sharpness(model, train)\n",
    "        sharpnesses[key + (batch_size,)] = sharpness_bs\n",
    "\n",
    "        ml_utils.save_sharpnesses_dict(sharpnesses)\n",
    "\n",
    "if not RETRAIN_FLAG:\n",
    "    sharpnesses = ml_utils.load_sharpnesses_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_sharpness(batch_sizes, sharpnesses, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance from initial weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_distance_from_initial_weight(models, initial_weights, batch_sizes, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_times, convergence_epochs, overall_training_times = visualization.extract_times_for_batch_sizes(models_states,\n",
    "                                                                                                     batch_sizes, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "visualization.plot_mean_time_per_epoch(batch_sizes, mean_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "visualization.histogram_num_of_train_epochs_until_conv(batch_sizes, convergence_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "visualization.histogram_overall_time_until_end_of_epochs(batch_sizes, overall_training_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_sharpness_times_runtime(batch_sizes, overall_training_times, sharpnesses, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Different batch sizes with linear increasing learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## with sharpness aware minimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### SGD with Momentum + SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.001, 0.002, 0.004, 0.008, 0.016, 0.032]\n",
    "batch_sizes = [32, 64, 128, 256, 512, 1024]\n",
    "training_epochs = EPOCHS\n",
    "\n",
    "key = ('increasing', 'sgd', 'sam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_by_batch_size_diff_lr = {}\n",
    "model_history_dict_diff_lr = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_lr_state_filename = 'model_diff_lr_diff_bs_state'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not RETRAIN_FLAG:\n",
    "    for batch_size, lr in zip(batch_sizes, learning_rates):\n",
    "        # Read training data\n",
    "        train, validation = data_loading.load_batched_and_resized_dataset(dataset_name=dataset_name,\n",
    "                                                                          batch_size=batch_size,\n",
    "                                                                          img_size=32)\n",
    "        optimizer = keras.optimizers.SGD(learning_rate=lr, momentum=0.9)\n",
    "        model = build_and_load_sam_model_weights(train, optimizer,\n",
    "                                                 filename_bs_lr_opt_sam(batch_size, lr, \"SGD-MOM\", \"SAM\"))\n",
    "        models[key + (batch_size, lr)] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if RETRAIN_FLAG:\n",
    "    models_states[key] = {}\n",
    "    for batch_size, lr in zip(batch_sizes, learning_rates):\n",
    "        with tf.distribute.MirroredStrategy().scope():\n",
    "            # Read training data\n",
    "            train, validation = data_loading.load_batched_and_resized_dataset(dataset_name=dataset_name,\n",
    "                                                                              batch_size=batch_size,\n",
    "                                                                              img_size=32)\n",
    "\n",
    "            optimizer = keras.optimizers.SGD(learning_rate=lr, momentum=0.9)\n",
    "            model = tf_models.build_simple_cnn_sam(train, optimizer, adaptive=True, rho=2.0)\n",
    "            train_callbacks = callback_for_filename(filename_bs_lr_opt_sam(batch_size, lr, \"SGD-MOM\", \"SAM\"))\n",
    "\n",
    "            models_states[key][batch_size] = ml_utils.train_model(model, train, validation, epochs=training_epochs,\n",
    "                                                                  extra_callbacks=train_callbacks, verbose=1)\n",
    "            models[key + (batch_size, lr)] = model\n",
    "            initial_weights[key + (batch_size, lr)] = model.get_weights()\n",
    "            ml_utils.save_initial_weights_dict(initial_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN_FLAG:\n",
    "    ml_utils.save_model_state(models_states[key], diff_lr_state_filename)\n",
    "else:\n",
    "    models_states[key] = ml_utils.load_model_state(diff_lr_state_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_loss_by_param(models_states[key], 'batch size with increasing learning rate',\n",
    "                                 'diff_lr_diff_bs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Sharpness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN_FLAG:\n",
    "    for batch_size, lr in zip(batch_sizes, learning_rates):\n",
    "        model = models[key + (batch_size, lr)]\n",
    "\n",
    "        sharpness = visualization.get_sharpness(model.base_model, train)\n",
    "        sharpnesses[key + (batch_size, lr)] = sharpness\n",
    "\n",
    "        ml_utils.save_sharpnesses_dict(sharpnesses)\n",
    "\n",
    "if not RETRAIN_FLAG:\n",
    "    sharpnesses = ml_utils.load_sharpnesses_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_sharpness(batch_sizes, sharpnesses, key, learning_rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance from initial weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_distance_from_initial_weight(models, initial_weights, batch_sizes, key, learning_rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean_times, convergence_epochs, overall_training_times = visualization.extract_times_for_batch_sizes(models_states,\n",
    "                                                                                                     batch_sizes, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "visualization.plot_mean_time_per_epoch(batch_sizes, mean_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.histogram_num_of_train_epochs_until_conv(batch_sizes, convergence_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.histogram_overall_time_until_end_of_epochs(batch_sizes, overall_training_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Save to Drive in case we run on Google Colab\n",
    "if USE_COLAB:\n",
    "    !cp -r /content/graphs/ /content/drive/MyDrive/\n",
    "    !cp -r /content/tmp/ /content/drive/MyDrive/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### SGD with Momentum + ASAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.001, 0.002, 0.004, 0.008, 0.016, 0.032]\n",
    "batch_sizes = [32, 64, 128, 256, 512, 1024]\n",
    "training_epochs = EPOCHS\n",
    "\n",
    "key = ('increasing', 'sgd', 'asam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_filename = 'model_diff_lr_diff_bs_sgd_asam_state'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not RETRAIN_FLAG:\n",
    "    for batch_size, lr in zip(batch_sizes, learning_rates):\n",
    "        # Read training data\n",
    "        train, validation = data_loading.load_batched_and_resized_dataset(dataset_name=dataset_name,\n",
    "                                                                          batch_size=batch_size,\n",
    "                                                                          img_size=32)\n",
    "        optimizer = keras.optimizers.SGD(learning_rate=lr, momentum=0.9)\n",
    "        model = build_and_load_sam_model_weights(train, optimizer,\n",
    "                                                 filename_bs_lr_opt_sam(batch_size, lr, \"SGD-MOM\", \"ASAM\"))\n",
    "        models[key + (batch_size, lr)] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if RETRAIN_FLAG:\n",
    "    models_states[key] = {}\n",
    "    for batch_size, lr in zip(batch_sizes, learning_rates):\n",
    "        with tf.distribute.MirroredStrategy().scope():\n",
    "            # Read training data\n",
    "            train, validation = data_loading.load_batched_and_resized_dataset(dataset_name=dataset_name,\n",
    "                                                                              batch_size=batch_size,\n",
    "                                                                              img_size=32)\n",
    "\n",
    "            optimizer = keras.optimizers.SGD(learning_rate=lr, momentum=0.9)\n",
    "            model = tf_models.build_simple_cnn_sam(train, optimizer, adaptive=True, rho=2.0)\n",
    "            train_callbacks = callback_for_filename(filename_bs_lr_opt_sam(batch_size, lr, \"SGD-MOM\", \"ASAM\"))\n",
    "\n",
    "            models_states[key][batch_size] = ml_utils.train_model(model, train, validation, epochs=training_epochs,\n",
    "                                                                  extra_callbacks=train_callbacks, verbose=1)\n",
    "            models[key + (batch_size, lr)] = model\n",
    "            initial_weights[key + (batch_size, lr)] = model.get_weights()\n",
    "            ml_utils.save_initial_weights_dict(initial_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN_FLAG:\n",
    "    ml_utils.save_model_state(models_states[key], state_filename)\n",
    "else:\n",
    "    models_states[key] = ml_utils.load_model_state(state_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_loss_by_param(models_states[key], 'batch size with increasing learning rate with SGD and ASAM',\n",
    "                                 'diff_lr_diff_bs_SGD_ASAM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sharpness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN_FLAG:\n",
    "    for batch_size, lr in zip(batch_sizes, learning_rates):\n",
    "        model = models[key + (batch_size, lr)]\n",
    "\n",
    "        sharpness = visualization.get_sharpness(model.base_model, train)\n",
    "        sharpnesses[key + (batch_size, lr)] = sharpness\n",
    "\n",
    "        ml_utils.save_sharpnesses_dict(sharpnesses)\n",
    "\n",
    "if not RETRAIN_FLAG:\n",
    "    sharpnesses = ml_utils.load_sharpnesses_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_sharpness(batch_sizes, sharpnesses, key, learning_rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance from initial weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_distance_from_initial_weight(models, initial_weights, batch_sizes, key, learning_rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_times, convergence_epochs, overall_training_times = visualization.extract_times_for_batch_sizes(models_states,\n",
    "                                                                                                     batch_sizes, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "visualization.plot_mean_time_per_epoch(batch_sizes, mean_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.histogram_num_of_train_epochs_until_conv(batch_sizes, convergence_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.histogram_overall_time_until_end_of_epochs(batch_sizes, overall_training_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Save to Drive in case we run on Google Colab\n",
    "if USE_COLAB:\n",
    "    !cp -r /content/graphs/ /content/drive/MyDrive/\n",
    "    !cp -r /content/tmp/ /content/drive/MyDrive/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### ADAM + SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learning_rates = [0.001, 0.002, 0.004, 0.008, 0.016, 0.032]\n",
    "batch_sizes = [32, 64, 128, 256, 512, 1024]\n",
    "training_epochs = EPOCHS\n",
    "\n",
    "key = ('increasing', 'adam', 'sam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_filename = 'model_diff_lr_diff_bs_adam_sam_state'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not RETRAIN_FLAG:\n",
    "    for batch_size, lr in zip(batch_sizes, learning_rates):\n",
    "        # Read training data\n",
    "        train, validation = data_loading.load_batched_and_resized_dataset(dataset_name=dataset_name,\n",
    "                                                                          batch_size=batch_size,\n",
    "                                                                          img_size=32)\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "        model = build_and_load_sam_model_weights(train, optimizer,\n",
    "                                                 filename_bs_lr_opt_sam(batch_size, lr, \"ADAM\", \"SAM\"))\n",
    "        models[key + (batch_size, lr)] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if RETRAIN_FLAG:\n",
    "    models_states[key] = {}\n",
    "    for batch_size, lr in zip(batch_sizes, learning_rates):\n",
    "        with tf.distribute.MirroredStrategy().scope():\n",
    "            # Read training data\n",
    "            train, validation = data_loading.load_batched_and_resized_dataset(dataset_name=dataset_name,\n",
    "                                                                              batch_size=batch_size,\n",
    "                                                                              img_size=32)\n",
    "\n",
    "            optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "            model = tf_models.build_simple_cnn_sam(train, optimizer)\n",
    "            train_callbacks = callback_for_filename(filename_bs_lr_opt_sam(batch_size, lr, \"ADAM\", \"SAM\"))\n",
    "\n",
    "            models_states[key][batch_size] = ml_utils.train_model(model, train, validation, epochs=training_epochs,\n",
    "                                                                  extra_callbacks=train_callbacks, verbose=1)\n",
    "            models[key + (batch_size, lr)] = model\n",
    "            initial_weights[key + (batch_size, lr)] = model.get_weights()\n",
    "            ml_utils.save_initial_weights_dict(initial_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN_FLAG:\n",
    "    ml_utils.save_model_state(models_states[key], state_filename)\n",
    "else:\n",
    "    models_states[key] = ml_utils.load_model_state(state_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_loss_by_param(models_states[key], 'batch size with increasing learning rate with ADAM and SAM',\n",
    "                                 'diff_lr_diff_bs_ADAM_SAM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sharpness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN_FLAG:\n",
    "    for batch_size, lr in zip(batch_sizes, learning_rates):\n",
    "        model = models[key + (batch_size, lr)]\n",
    "\n",
    "        sharpness = visualization.get_sharpness(model.base_model, train)\n",
    "        sharpnesses[key + (batch_size, lr)] = sharpness\n",
    "\n",
    "        ml_utils.save_sharpnesses_dict(sharpnesses)\n",
    "\n",
    "if not RETRAIN_FLAG:\n",
    "    sharpnesses = ml_utils.load_sharpnesses_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_sharpness(batch_sizes, sharpnesses, key, learning_rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance from initial weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_distance_from_initial_weight(models, initial_weights, batch_sizes, key, learning_rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_times, convergence_epochs, overall_training_times = visualization.extract_times_for_batch_sizes(models_states,\n",
    "                                                                                                     batch_sizes, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "visualization.plot_mean_time_per_epoch(batch_sizes, mean_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.histogram_num_of_train_epochs_until_conv(batch_sizes, convergence_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.histogram_overall_time_until_end_of_epochs(batch_sizes, overall_training_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Save to Drive in case we run on Google Colab\n",
    "if USE_COLAB:\n",
    "    !cp -r /content/graphs/ /content/drive/MyDrive/\n",
    "    !cp -r /content/tmp/ /content/drive/MyDrive/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## ADAM + ASAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.001, 0.002, 0.004, 0.008, 0.016, 0.032]\n",
    "batch_sizes = [32, 64, 128, 256, 512, 1024]\n",
    "training_epochs = EPOCHS\n",
    "\n",
    "key = ('increasing', 'adam', 'asam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_filename = 'model_diff_lr_diff_bs_adam_asam_state'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not RETRAIN_FLAG:\n",
    "    for batch_size, lr in zip(batch_sizes, learning_rates):\n",
    "        # Read training data\n",
    "        train, validation = data_loading.load_batched_and_resized_dataset(dataset_name=dataset_name,\n",
    "                                                                          batch_size=batch_size,\n",
    "                                                                          img_size=32)\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "        model = build_and_load_sam_model_weights(train, optimizer,\n",
    "                                                 filename_bs_lr_opt_sam(batch_size, lr, \"ADAM\", \"ASAM\"))\n",
    "        models[key + (batch_size, lr)] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if RETRAIN_FLAG:\n",
    "    models_states[key] = {}\n",
    "    for batch_size, lr in zip(batch_sizes, learning_rates):\n",
    "        with tf.distribute.MirroredStrategy().scope():\n",
    "            # Read training data\n",
    "            train, validation = data_loading.load_batched_and_resized_dataset(dataset_name=dataset_name,\n",
    "                                                                              batch_size=batch_size,\n",
    "                                                                              img_size=32)\n",
    "\n",
    "            optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "            model = tf_models.build_simple_cnn_sam(train, optimizer)\n",
    "            train_callbacks = callback_for_filename(filename_bs_lr_opt_sam(batch_size, lr, \"ADAM\", \"ASAM\"))\n",
    "\n",
    "            models_states[key][batch_size] = ml_utils.train_model(model, train, validation, epochs=training_epochs,\n",
    "                                                                  extra_callbacks=train_callbacks, verbose=1)\n",
    "            models[key + (batch_size, lr)] = model\n",
    "            initial_weights[key + (batch_size, lr)] = model.get_weights()\n",
    "            ml_utils.save_initial_weights_dict(initial_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN_FLAG:\n",
    "    ml_utils.save_model_state(models_states[key], state_filename)\n",
    "else:\n",
    "    models_states[key] = ml_utils.load_model_state(state_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_loss_by_param(models_states[key], 'batch size with increasing learning rate with ADAM and ASAM',\n",
    "                                 'diff_lr_diff_bs_ADAM_ASAM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sharpness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN_FLAG:\n",
    "    for batch_size, lr in zip(batch_sizes, learning_rates):\n",
    "        model = models[key + (batch_size, lr)]\n",
    "\n",
    "        sharpness = visualization.get_sharpness(model.base_model, train)\n",
    "        sharpnesses[key + (batch_size, lr)] = sharpness\n",
    "\n",
    "        ml_utils.save_sharpnesses_dict(sharpnesses)\n",
    "\n",
    "if not RETRAIN_FLAG:\n",
    "    sharpnesses = ml_utils.load_sharpnesses_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_sharpness(batch_sizes, sharpnesses, key, learning_rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance from initial weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_distance_from_initial_weight(models, initial_weights, batch_sizes, key, learning_rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean_times, convergence_epochs, overall_training_times = visualization.extract_times_for_batch_sizes(models_states,\n",
    "                                                                                                     batch_sizes, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "visualization.plot_mean_time_per_epoch(batch_sizes, mean_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "visualization.histogram_num_of_train_epochs_until_conv(batch_sizes, convergence_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.histogram_overall_time_until_end_of_epochs(batch_sizes, overall_training_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Save to Drive in case we run on Google Colab\n",
    "if USE_COLAB:\n",
    "    !cp -r /content/graphs/ /content/drive/MyDrive/\n",
    "    !cp -r /content/tmp/ /content/drive/MyDrive/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## without sharpness aware minimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### SGD with Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learning_rates = [0.001, 0.002, 0.004, 0.008, 0.016, 0.032]\n",
    "batch_sizes = [32, 64, 128, 256, 512, 1024]\n",
    "training_epochs = EPOCHS\n",
    "\n",
    "key = ('increasing', 'SGD', 'none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_filename = 'model_diff_lr_diff_bs_sgd_state'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if not RETRAIN_FLAG:\n",
    "    for batch_size, lr in zip(batch_sizes, learning_rates):\n",
    "        # Read training data\n",
    "        train, validation = data_loading.load_batched_and_resized_dataset(dataset_name=dataset_name,\n",
    "                                                                          batch_size=batch_size,\n",
    "                                                                          img_size=32)\n",
    "        optimizer = keras.optimizers.SGD(learning_rate=lr, momentum=0.9)\n",
    "        model = build_and_load_model_weights(train, optimizer,\n",
    "                                             filename_bs_lr_opt_sam(batch_size, lr, \"SGD\", \"NONE\"))\n",
    "        models[key + (batch_size, lr)] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN_FLAG:\n",
    "    models_states[key] = {}\n",
    "    for batch_size, lr in zip(batch_sizes, learning_rates):\n",
    "        with tf.distribute.MirroredStrategy().scope():\n",
    "            # Read training data\n",
    "            train, validation = data_loading.load_batched_and_resized_dataset(dataset_name=dataset_name,\n",
    "                                                                              batch_size=batch_size,\n",
    "                                                                              img_size=32)\n",
    "\n",
    "            optimizer = keras.optimizers.SGD(learning_rate=lr, momentum=0.9)\n",
    "            model = tf_models.build_and_compile_simple_cnn(train, optimizer)\n",
    "            train_callbacks = callback_for_filename(filename_bs_lr_opt_sam(batch_size, lr, \"SGD\", \"NONE\"))\n",
    "\n",
    "            models_states[key][batch_size] = ml_utils.train_model(model, train, validation, epochs=training_epochs,\n",
    "                                                                  extra_callbacks=train_callbacks, verbose=1)\n",
    "            models[key + (batch_size, lr)] = model\n",
    "            initial_weights[key + (batch_size, lr)] = model.get_weights()\n",
    "            ml_utils.save_initial_weights_dict(initial_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN_FLAG:\n",
    "    ml_utils.save_model_state(models_states[key], state_filename)\n",
    "else:\n",
    "    models_states[key] = ml_utils.load_model_state(state_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_loss_by_param(models_states[key], 'batch size with increasing learning rate with SGD',\n",
    "                                 'diff_lr_diff_bs_SGD_NONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sharpness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN_FLAG:\n",
    "    for batch_size, lr in zip(batch_sizes, learning_rates):\n",
    "        model = models[key + (batch_size, lr)]\n",
    "\n",
    "        sharpness = visualization.get_sharpness(model, train)\n",
    "        sharpnesses[key + (batch_size, lr)] = sharpness\n",
    "\n",
    "        ml_utils.save_sharpnesses_dict(sharpnesses)\n",
    "\n",
    "if not RETRAIN_FLAG:\n",
    "    sharpnesses = ml_utils.load_sharpnesses_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_sharpness(batch_sizes, sharpnesses, key, learning_rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance from initial weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_distance_from_initial_weight(models, initial_weights, batch_sizes, key, learning_rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_times, convergence_epochs, overall_training_times = visualization.extract_times_for_batch_sizes(models_states,\n",
    "                                                                                                     batch_sizes, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "visualization.plot_mean_time_per_epoch(batch_sizes, mean_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.histogram_num_of_train_epochs_until_conv(batch_sizes, convergence_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.histogram_overall_time_until_end_of_epochs(batch_sizes, overall_training_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Save to Drive in case we run on Google Colab\n",
    "if USE_COLAB:\n",
    "    !cp -r /content/graphs/ /content/drive/MyDrive/\n",
    "    !cp -r /content/tmp/ /content/drive/MyDrive/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.001, 0.002, 0.004, 0.008, 0.016, 0.032]\n",
    "batch_sizes = [32, 64, 128, 256, 512, 1024]\n",
    "training_epochs = EPOCHS\n",
    "\n",
    "key = ('increasing', 'adam', 'none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_filename = 'model_diff_lr_diff_bs_adam_state'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if not RETRAIN_FLAG:\n",
    "    for batch_size, lr in zip(batch_sizes, learning_rates):\n",
    "        # Read training data\n",
    "        train, validation = data_loading.load_batched_and_resized_dataset(dataset_name=dataset_name,\n",
    "                                                                          batch_size=batch_size,\n",
    "                                                                          img_size=32)\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "        model = build_and_load_model_weights(train, optimizer,\n",
    "                                             filename_bs_lr_opt_sam(batch_size, lr, \"ADAM\", \"NONE\"))\n",
    "        models[key + (batch_size, lr)] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN_FLAG:\n",
    "    models_states[key] = {}\n",
    "    for batch_size, lr in zip(batch_sizes, learning_rates):\n",
    "        with tf.distribute.MirroredStrategy().scope():\n",
    "            # Read training data\n",
    "            train, validation = data_loading.load_batched_and_resized_dataset(dataset_name=dataset_name,\n",
    "                                                                              batch_size=batch_size,\n",
    "                                                                              img_size=32)\n",
    "\n",
    "            optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "            model = tf_models.build_and_compile_simple_cnn(train, optimizer)\n",
    "            train_callbacks = callback_for_filename(filename_bs_lr_opt_sam(batch_size, lr, \"ADAM\", \"NONE\"))\n",
    "\n",
    "            models_states[key][batch_size] = ml_utils.train_model(model, train, validation, epochs=training_epochs,\n",
    "                                                                  extra_callbacks=train_callbacks, verbose=1)\n",
    "            models[key + (batch_size, lr)] = model\n",
    "            initial_weights[key + (batch_size, lr)] = model.get_weights()\n",
    "            ml_utils.save_initial_weights_dict(initial_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN_FLAG:\n",
    "    ml_utils.save_model_state(models_states[key], state_filename)\n",
    "else:\n",
    "    models_states[key] = ml_utils.load_model_state(state_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_loss_by_param(models_states[key], 'batch size with increasing learning rate with ADAM',\n",
    "                                 'diff_lr_diff_bs_ADAM_NONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sharpness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if RETRAIN_FLAG:\n",
    "    for batch_size, lr in zip(batch_sizes, learning_rates):\n",
    "        model = models[key + (batch_size, lr)]\n",
    "\n",
    "        sharpness = visualization.get_sharpness(model, train)\n",
    "        sharpnesses[key + (batch_size, lr)] = sharpness\n",
    "\n",
    "        ml_utils.save_sharpnesses_dict(sharpnesses)\n",
    "\n",
    "if not RETRAIN_FLAG:\n",
    "    sharpnesses = ml_utils.load_sharpnesses_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "visualization.plot_sharpness(batch_sizes, sharpnesses, key, learning_rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance from initial weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_distance_from_initial_weight(models, initial_weights, batch_sizes, key, learning_rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_times, convergence_epochs, overall_training_times = visualization.extract_times_for_batch_sizes(models_states,\n",
    "                                                                                                     batch_sizes, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_mean_time_per_epoch(batch_sizes, mean_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.histogram_num_of_train_epochs_until_conv(batch_sizes, convergence_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.histogram_overall_time_until_end_of_epochs(batch_sizes, overall_training_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_sharpness_times_runtime(batch_sizes, overall_training_times, sharpnesses, key, learning_rates)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
