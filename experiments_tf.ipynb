{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Google colab initialization\n",
    "\n",
    "For Google colab uncomment these lines and run them to access your drive or try the second way (not tested)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# import sys\n",
    "#\n",
    "# sys.path.insert(1, r'/content/drive/My Drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Other try\n",
    "# !git clone https://github.com/Alexanderstaehle/OM_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append(\"OM_project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from utils import ml_utils, visualization, data_loading, tf_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Number of accelerators:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-20 17:33:37.358311: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-20 17:33:37.383737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-20 17:33:37.383936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-20 17:33:37.414919: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-20 17:33:37.415750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-20 17:33:37.415934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-20 17:33:37.416076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-20 17:33:37.798251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-20 17:33:37.798430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-20 17:33:37.798560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-20 17:33:37.798671: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5881 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "data_loading.initialize_env()\n",
    "sns.set_theme()\n",
    "color_map = sns.color_palette(as_cmap=True)\n",
    "ml_utils.check_tpu_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different batch sizes with fixed learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## without sharpness aware minimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD with Momemtum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "models_dict_fixed = {}\n",
    "#batch_sizes = [32, 64, 128, 256, 512, 1024]\n",
    "batch_sizes = [32]\n",
    "lr = 0.001\n",
    "training_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test of SAM model \n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train, validation = data_loading.load_batched_and_resized_dataset(dataset_name='MNIST', batch_size=batch_size,\n",
    "                                                                 img_size=32)\n",
    "\n",
    "#optimizer = keras.optimizers.SGD(learning_rate=lr)\n",
    "optimizer = keras.optimizers.SGD(learning_rate=lr, momentum=0.9)\n",
    "#base_model = tf_models.build_simple_dense_model(train)\n",
    "base_model = tf_models.build_simple_cnn(train)\n",
    "base_model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "train_callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\", patience=10,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        # tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        #     monitor=\"val_loss\", factor=0.5,\n",
    "        #     patience=3, verbose=1\n",
    "        # )\n",
    "    ]\n",
    "model = tf_models.SAMModel(base_model, adaptive = True, rho = 2.0)    \n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "models_dict_fixed[batch_size] = ml_utils.train_model(model, train, validation, epochs=1,\n",
    "                                                         extra_callbacks=train_callbacks, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_bs = lambda bs: ml_utils.path_from_filename(f'model_fixed_lr_diff_bs_{bs}', format_ = \"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-20 17:34:18.526181: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8302\n",
      "2022-02-20 17:34:19.014288: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1499/1500 [============================>.] - ETA: 0s - loss: 0.8057 - accuracy: 0.7262\n",
      "Epoch 1: val_loss improved from inf to 0.24667, saving model to tmp/model_fixed_lr_diff_bs_32.tf\n",
      "1500/1500 [==============================] - 7s 4ms/step - loss: 0.8054 - accuracy: 0.7263 - val_loss: 0.2467 - val_accuracy: 0.9347\n",
      "Epoch 2/10\n",
      "1494/1500 [============================>.] - ETA: 0s - loss: 0.2559 - accuracy: 0.9090\n",
      "Epoch 2: val_loss improved from 0.24667 to 0.15793, saving model to tmp/model_fixed_lr_diff_bs_32.tf\n",
      "1500/1500 [==============================] - 10s 6ms/step - loss: 0.2557 - accuracy: 0.9090 - val_loss: 0.1579 - val_accuracy: 0.9585\n",
      "Epoch 3/10\n",
      "1499/1500 [============================>.] - ETA: 0s - loss: 0.1969 - accuracy: 0.9277\n",
      "Epoch 3: val_loss improved from 0.15793 to 0.12920, saving model to tmp/model_fixed_lr_diff_bs_32.tf\n",
      "1500/1500 [==============================] - 13s 9ms/step - loss: 0.1969 - accuracy: 0.9277 - val_loss: 0.1292 - val_accuracy: 0.9655\n",
      "Epoch 4/10\n",
      "1495/1500 [============================>.] - ETA: 0s - loss: 0.1706 - accuracy: 0.9359\n",
      "Epoch 4: val_loss improved from 0.12920 to 0.11296, saving model to tmp/model_fixed_lr_diff_bs_32.tf\n",
      "1500/1500 [==============================] - 12s 8ms/step - loss: 0.1706 - accuracy: 0.9359 - val_loss: 0.1130 - val_accuracy: 0.9694\n",
      "Epoch 5/10\n",
      "1494/1500 [============================>.] - ETA: 0s - loss: 0.1538 - accuracy: 0.9412\n",
      "Epoch 5: val_loss improved from 0.11296 to 0.10204, saving model to tmp/model_fixed_lr_diff_bs_32.tf\n",
      "1500/1500 [==============================] - 12s 8ms/step - loss: 0.1537 - accuracy: 0.9413 - val_loss: 0.1020 - val_accuracy: 0.9726\n",
      "Epoch 6/10\n",
      "1496/1500 [============================>.] - ETA: 0s - loss: 0.1417 - accuracy: 0.9458\n",
      "Epoch 6: val_loss improved from 0.10204 to 0.09368, saving model to tmp/model_fixed_lr_diff_bs_32.tf\n",
      "1500/1500 [==============================] - 9s 6ms/step - loss: 0.1417 - accuracy: 0.9458 - val_loss: 0.0937 - val_accuracy: 0.9745\n",
      "Epoch 7/10\n",
      "1499/1500 [============================>.] - ETA: 0s - loss: 0.1324 - accuracy: 0.9487\n",
      "Epoch 7: val_loss improved from 0.09368 to 0.08704, saving model to tmp/model_fixed_lr_diff_bs_32.tf\n",
      "1500/1500 [==============================] - 12s 8ms/step - loss: 0.1324 - accuracy: 0.9487 - val_loss: 0.0870 - val_accuracy: 0.9762\n",
      "Epoch 8/10\n",
      "1496/1500 [============================>.] - ETA: 0s - loss: 0.1250 - accuracy: 0.9512\n",
      "Epoch 8: val_loss improved from 0.08704 to 0.08180, saving model to tmp/model_fixed_lr_diff_bs_32.tf\n",
      "1500/1500 [==============================] - 11s 7ms/step - loss: 0.1250 - accuracy: 0.9513 - val_loss: 0.0818 - val_accuracy: 0.9775\n",
      "Epoch 9/10\n",
      "1497/1500 [============================>.] - ETA: 0s - loss: 0.1188 - accuracy: 0.9534\n",
      "Epoch 9: val_loss improved from 0.08180 to 0.07748, saving model to tmp/model_fixed_lr_diff_bs_32.tf\n",
      "1500/1500 [==============================] - 13s 8ms/step - loss: 0.1189 - accuracy: 0.9534 - val_loss: 0.0775 - val_accuracy: 0.9788\n",
      "Epoch 10/10\n",
      "1499/1500 [============================>.] - ETA: 0s - loss: 0.1138 - accuracy: 0.9551\n",
      "Epoch 10: val_loss improved from 0.07748 to 0.07377, saving model to tmp/model_fixed_lr_diff_bs_32.tf\n",
      "1500/1500 [==============================] - 10s 7ms/step - loss: 0.1138 - accuracy: 0.9551 - val_loss: 0.0738 - val_accuracy: 0.9797\n"
     ]
    }
   ],
   "source": [
    "# Need sparse categorical crossentropy since our labels are in form of integers not vectors\n",
    "for batch_size in batch_sizes:\n",
    "    # Read training data\n",
    "    train, validation = data_loading.load_batched_and_resized_dataset(dataset_name='MNIST', batch_size=batch_size,\n",
    "                                                                      img_size=32)\n",
    "    \n",
    "    optimizer = keras.optimizers.SGD(learning_rate=lr, momentum=0.9)\n",
    "    \n",
    "    base_model = tf_models.build_simple_cnn(train)\n",
    "    base_model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model = tf_models.SAMModel(base_model, adaptive = True, rho = 2.0)    \n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    train_callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\", patience=10,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filename_bs(batch_size),\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            verbose=1,\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True\n",
    "        )\n",
    "        # tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        #     monitor=\"val_loss\", factor=0.5,\n",
    "        #     patience=3, verbose=1\n",
    "        # )\n",
    "    ]\n",
    "    models_dict_fixed[batch_size] = ml_utils.train_model(model, train, validation, epochs=10,\n",
    "                                                         extra_callbacks=train_callbacks, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_lr_state_filename = 'model_fixed_lr_diff_bs_state'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ml_utils.save_model_state(models_dict_fixed, fixed_lr_state_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "visualization.plot_loss_by_param(models_dict_fixed, 'batch size with fixed learning rate', 'fixed_lr_diff_bs')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sharpness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(learning_rate=lr, momentum=0.9)\n",
    "\n",
    "model = tf_models.build_simple_cnn_sam(train, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_by_batch_size_fixed_lr = model.load_weights(filename_bs(\"32\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_utils.load_model_state(fixed_lr_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.0652 - accuracy: 0.9827\n",
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.0652 - accuracy: 0.9827\n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =        59786     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f= -6.52045D-02    |proj g|=  1.47881D-02\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 1.6081 - accuracy: 0.5088\n",
      "\n",
      "At iterate    1    f= -1.60812D+00    |proj g|=  2.85367D-02\n",
      "  ys=-3.182E+03  -gs= 1.657E+02 BFGS update SKIPPED\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 7.3744 - accuracy: 0.4440\n",
      "\n",
      "At iterate    2    f= -7.37442D+00    |proj g|=  2.95762D-02\n",
      "  ys=-1.628E+04  -gs= 2.063E+03 BFGS update SKIPPED\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 12.9446 - accuracy: 0.3739\n",
      "\n",
      "At iterate    3    f= -1.29446D+01    |proj g|=  2.34495D-02\n",
      "  ys=-1.048E+04  -gs= 3.084E+03 BFGS update SKIPPED\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 15.1740 - accuracy: 0.3871\n",
      "\n",
      "At iterate    4    f= -1.51740D+01    |proj g|=  2.43990D-02\n",
      "  ys=-3.679E+03  -gs= 1.475E+03 BFGS update SKIPPED\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 16.6390 - accuracy: 0.3886\n",
      "\n",
      "At iterate    5    f= -1.66390D+01    |proj g|=  2.42708D-02\n",
      "  ys=-2.820E+03  -gs= 8.582E+02 BFGS update SKIPPED\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 17.5588 - accuracy: 0.3867\n",
      "\n",
      "At iterate    6    f= -1.75588D+01    |proj g|=  2.34562D-02\n",
      "  ys=-2.143E+03  -gs= 4.229E+02 BFGS update SKIPPED\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 18.0616 - accuracy: 0.3886\n",
      "\n",
      "At iterate    7    f= -1.80616D+01    |proj g|=  2.28355D-02\n",
      "  ys=-8.549E+02  -gs= 3.558E+02 BFGS update SKIPPED\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 19.0138 - accuracy: 0.3895\n",
      "\n",
      "At iterate    8    f= -1.90138D+01    |proj g|=  2.42816D-02\n",
      "  ys=-2.169E+03  -gs= 3.672E+02 BFGS update SKIPPED\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 19.3677 - accuracy: 0.3885\n",
      "\n",
      "At iterate    9    f= -1.93677D+01    |proj g|=  2.28409D-02\n",
      "  ys=-8.494E+02  -gs= 1.465E+02 BFGS update SKIPPED\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 19.5439 - accuracy: 0.3890\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1828.6382814730484"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   10    f= -1.95439D+01    |proj g|=  2.22052D-02\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "59786     10     11  80715     9 58386   2.221D-02  -1.954D+01\n",
      "  F =  -19.543941497802734     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n"
     ]
    }
   ],
   "source": [
    "visualization.get_sharpness(model.base_model, train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- loss curve\n",
    "- val loss curve\n",
    "- final train and val loss (best) \n",
    "- sharpness of the minimizers \n",
    "- distance from initial weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- training time\n",
    "- training time per epoch \n",
    "- epochs needed to converge \n",
    "- parallelization \n",
    "- training time times sharpness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with sharpness aware minimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD with Momentum + SAM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD with Momentum + ASAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADAM + SAM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADAM + ASAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Different batch sizes with linear increasing learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "learning_rates = [0.001, 0.002, 0.004, 0.008, 0.016, 0.032]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
