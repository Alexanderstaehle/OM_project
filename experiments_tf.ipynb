{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Google colab initialization\n",
    "\n",
    "For Google colab uncomment these lines and run them to access your drive or try the second way (not tested)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# import sys\n",
    "#\n",
    "# sys.path.insert(1, r'/content/drive/My Drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Other try\n",
    "# !git clone https://github.com/Alexanderstaehle/OM_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append(\"OM_project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-19 13:36:31.890918: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-02-19 13:36:31.890934: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from utils import ml_utils, visualization, data_loading, tf_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "Number of accelerators:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-19 13:36:33.145485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-19 13:36:33.146317: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-02-19 13:36:33.146453: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-02-19 13:36:33.146572: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-02-19 13:36:33.146690: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-02-19 13:36:33.146806: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-02-19 13:36:33.146922: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-02-19 13:36:33.147037: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-02-19 13:36:33.147149: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-02-19 13:36:33.147169: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-02-19 13:36:33.147711: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "data_loading.initialize_env()\n",
    "sns.set_theme()\n",
    "color_map = sns.color_palette(as_cmap=True)\n",
    "ml_utils.check_tpu_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different batch sizes with fixed learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "models_dict_fixed = {}\n",
    "batch_sizes = [32, 64, 128, 256, 512, 1024]\n",
    "# batch_sizes = [32, 64, 128]\n",
    "lr = 0.001\n",
    "training_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 20s 13ms/step - loss: 1.9340 - accuracy: 0.3889 - val_loss: 1.2981 - val_accuracy: 0.7424\n"
     ]
    }
   ],
   "source": [
    "# test of SAM model \n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train, validation = data_loading.load_batched_and_resized_dataset(dataset_name='MNIST', batch_size=batch_size,\n",
    "                                                                 img_size=32)\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=lr)\n",
    "#base_model = tf_models.build_simple_dense_model(train)\n",
    "base_model = tf_models.build_simple_cnn(train)\n",
    "base_model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "train_callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\", patience=10,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        # tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        #     monitor=\"val_loss\", factor=0.5,\n",
    "        #     patience=3, verbose=1\n",
    "        # )\n",
    "    ]\n",
    "model = tf_models.SAMModel(base_model, adaptive = True, rho = 2.0)    \n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "#model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#models_dict_fixed[batch_size] = ml_utils.train_model(model, train, validation, epochs=1,\n",
    "#                                                         extra_callbacks=train_callbacks, verbose=1)\n",
    "models_dict_fixed[batch_size] = ml_utils.train_model(base_model, train, validation, epochs=1,\n",
    "                                                         extra_callbacks=train_callbacks, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.set_weights(model.get_weights())\n",
    "base_model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "#base_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in train:\n",
    "    print(tf.argmax(base_model(x), axis = -1))\n",
    "    print(tf.convert_to_tensor(y, dtype=tf.float64))\n",
    "    print(tf.keras.losses.binary_crossentropy(y, tf.argmax(base_model(x), axis = -1)))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 5s 3ms/step - loss: 1.2936 - accuracy: 0.7446\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 1.2936 - accuracy: 0.7446\n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =        59786     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f= -1.29358D+00    |proj g|=  1.20978D-02\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 2.3028 - accuracy: 0.1021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " Line search cannot locate an adequate point after MAXLS\n",
      "  function and gradient evaluations.\n",
      "  Previous x, f and g restored.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "44.00377090533849"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "59786      1      2  53961     0 53960   1.210D-02  -1.294D+00\n",
      "  F =  -1.2935798168182373     \n",
      "\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH                              \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Possible causes: 1 error in function or gradient evaluation;\n",
      "                  2 rounding error dominate computation.\n"
     ]
    }
   ],
   "source": [
    "visualization.get_sharpness(base_model, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for batch_size in batch_sizes:\n",
    "    # Read training data\n",
    "    train, validation = data_loading.load_batched_and_resized_dataset(dataset_name='MNIST', batch_size=batch_size,\n",
    "                                                                      img_size=32)\n",
    "    model = tf_models.build_simple_dense_model(train)\n",
    "    train_callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\", patience=10,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        # tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        #     monitor=\"val_loss\", factor=0.5,\n",
    "        #     patience=3, verbose=1\n",
    "        # )\n",
    "    ]\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "    # Need sparse categorical crossentropy since our labels are in form of integers not vectors\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    models_dict_fixed[batch_size] = ml_utils.train_model(model, train, validation, epochs=10,\n",
    "                                                         extra_callbacks=train_callbacks, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ml_utils.save_model_state(models_dict_fixed, 'model_fixed_lr_diff_bs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "visualization.plot_loss_by_param(models_dict_fixed, 'batch size with fixed learning rate', 'fixed_lr_diff_bs')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Different batch sizes with linear increasing learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "learning_rates = [0.001, 0.002, 0.004, 0.008, 0.016, 0.032]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
