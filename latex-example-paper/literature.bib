
@misc{changEffectBatchSize2020,
  title = {Effect of {{Batch Size}} on {{Neural Net Training}}},
  author = {Chang, Daryl},
  year = {2020},
  month = aug,
  journal = {Deep Learning Experiments},
  abstract = {co-authored with Apurva Pathak},
  langid = {english},
  file = {C\:\\Users\\alexa\\Zotero\\storage\\ILCMKEQM\\effect-of-batch-size-on-neural-net-training-c5ae8516e57.html}
}

@article{dasDistributedDeepLearning2016,
  title = {Distributed {{Deep Learning Using Synchronous Stochastic Gradient Descent}}},
  author = {Das, Dipankar and Avancha, Sasikanth and Mudigere, Dheevatsa and Vaidynathan, Karthikeyan and Sridharan, Srinivas and Kalamkar, Dhiraj and Kaul, Bharat and Dubey, Pradeep},
  year = {2016},
  month = feb,
  journal = {arXiv:1602.06709 [cs]},
  eprint = {1602.06709},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We design and implement a distributed multinode synchronous SGD algorithm, without altering hyper parameters, or compressing data, or altering algorithmic behavior. We perform a detailed analysis of scaling, and identify optimal design points for different networks. We demonstrate scaling of CNNs on 100s of nodes, and present what we believe to be record training throughputs. A 512 minibatch VGG-A CNN training run is scaled 90X on 128 nodes. Also 256 minibatch VGG-A and OverFeat-FAST networks are scaled 53X and 42X respectively on a 64 node cluster. We also demonstrate the generality of our approach via best-in-class 6.5X scaling for a 7-layer DNN on 16 nodes. Thereafter we attempt to democratize deep-learning by training on an Ethernet based AWS cluster and show \textasciitilde 14X scaling on 16 nodes.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning},
  file = {C\:\\Users\\alexa\\Zotero\\storage\\E6EVVE6T\\Das et al_2016_Distributed Deep Learning Using Synchronous Stochastic Gradient Descent.pdf;C\:\\Users\\alexa\\Zotero\\storage\\WVGYU5LN\\1602.html}
}

@misc{foret2021sharpnessaware,
  title = {Sharpness-Aware Minimization for Efficiently Improving Generalization},
  author = {Foret, Pierre and Kleiner, Ariel and Mobahi, Hossein and Neyshabur, Behnam},
  year = {2021},
  eprint = {2010.01412},
  eprinttype = {arxiv},
  primaryclass = {cs.LG},
  archiveprefix = {arXiv}
}

@misc{goyal2018accurate,
  title = {Accurate, Large Minibatch {{SGD}}: {{Training ImageNet}} in 1 Hour},
  author = {Goyal, Priya and Doll{\'a}r, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  year = {2018},
  eprint = {1706.02677},
  eprinttype = {arxiv},
  primaryclass = {cs.CV},
  archiveprefix = {arXiv}
}

@misc{keskar2017largebatch,
  title = {On Large-Batch Training for Deep Learning: {{Generalization}} Gap and Sharp Minima},
  author = {Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  year = {2017},
  eprint = {1609.04836},
  eprinttype = {arxiv},
  primaryclass = {cs.LG},
  archiveprefix = {arXiv}
}

@inproceedings{kwonASAMAdaptiveSharpnessAware2021,
  title = {{{ASAM}}: {{Adaptive Sharpness-Aware Minimization}} for {{Scale-Invariant Learning}} of {{Deep Neural Networks}}},
  shorttitle = {{{ASAM}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Kwon, Jungmin and Kim, Jeongseop and Park, Hyunseo and Choi, In Kwon},
  year = {2021},
  month = jul,
  pages = {5905--5914},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Recently, learning algorithms motivated from sharpness of loss surface as an effective measure of generalization gap have shown state-of-the-art performances. Nevertheless, sharpness defined in a rigid region with a fixed radius, has a drawback in sensitivity to parameter re-scaling which leaves the loss unaffected, leading to weakening of the connection between sharpness and generalization gap. In this paper, we introduce the concept of adaptive sharpness which is scale-invariant and propose the corresponding generalization bound. We suggest a novel learning method, adaptive sharpness-aware minimization (ASAM), utilizing the proposed generalization bound. Experimental results in various benchmark datasets show that ASAM contributes to significant improvement of model generalization performance.},
  langid = {english},
  file = {C\:\\Users\\alexa\\Zotero\\storage\\7K9IY4HC\\Kwon et al_2021_ASAM.pdf;C\:\\Users\\alexa\\Zotero\\storage\\GYHMULUK\\Kwon et al. - 2021 - ASAM Adaptive Sharpness-Aware Minimization for Sc.pdf}
}

@incollection{lecunEfficientBackProp2012,
  title = {Efficient {{BackProp}}},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}: {{Second Edition}}},
  author = {LeCun, Yann A. and Bottou, L{\'e}on and Orr, Genevieve B. and M{\"u}ller, Klaus-Robert},
  editor = {Montavon, Gr{\'e}goire and Orr, Genevi{\`e}ve B. and M{\"u}ller, Klaus-Robert},
  year = {2012},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {9--48},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-35289-8_3},
  abstract = {The convergence of back-propagation learning is analyzed so as to explain common phenomenon observed by practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposed in serious technical publications. This paper gives some of those tricks, and offers explanations of why they work.Many authors have suggested that second-order optimization methods are advantageous for neural net training. It is shown that most ``classical'' second-order methods are impractical for large neural networks. A few methods are proposed that do not have these limitations.},
  isbn = {978-3-642-35289-8},
  langid = {english},
  keywords = {Conjugate Gradient,Gradient Descent,Handwritten Digit,Neural Information Processing System,Newton Algorithm}
}

@misc{li2018visualizing,
  title = {Visualizing the Loss Landscape of Neural Nets},
  author = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  year = {2018},
  eprint = {1712.09913},
  eprinttype = {arxiv},
  primaryclass = {cs.LG},
  archiveprefix = {arXiv}
}

@misc{paulSharpnessAwareMinimizationTensorFlow2022,
  title = {Sharpness-{{Aware-Minimization-TensorFlow}}},
  author = {Paul, Sayak},
  year = {2022},
  month = feb,
  abstract = {Implements sharpness-aware minimization (https://arxiv.org/abs/2010.01412) in TensorFlow 2.},
  keywords = {computer-vision,deep-neural-networks,generalization,loss-landscape,tensorflow,tpu-acceleration}
}

@misc{shenEffectBatchSize2018,
  title = {Effect of Batch Size on Training Dynamics},
  author = {Shen, Kevin},
  year = {2018},
  month = jun,
  journal = {Mini Distill},
  abstract = {This is a longer blogpost where I discuss results of experiments I ran myself.},
  langid = {english},
  file = {C\:\\Users\\alexa\\Zotero\\storage\\RXZTDX5Y\\effect-of-batch-size-on-training-dynamics-21c14f7a716e.html}
}


